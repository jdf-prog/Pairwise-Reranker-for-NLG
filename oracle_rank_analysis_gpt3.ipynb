{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset score consistent with PairReranker labels\n"
     ]
    }
   ],
   "source": [
    "\"\"\"GPT-3 testing\"\"\"\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from src.common.evaluation import METRIC_WEIGHTS\n",
    "\n",
    "\"\"\"CnnDM\"\"\"\n",
    "# dataset_name = 'cnndm_chatgpt'\n",
    "# set_name = 'test'\n",
    "# generation_methods=['top_p_sampling']\n",
    "# models = ['chatgpt']\n",
    "# metrics = ['rouge1', 'rouge2', 'rougeLsum']\n",
    "\"\"\"commongen\"\"\"\n",
    "dataset_name = 'commongen_chatgpt'\n",
    "set_name = 'val'\n",
    "generation_methods=['top_p_sampling']\n",
    "models = ['gpt-3.5-turbo']\n",
    "metrics = ['bleu', 'cider']\n",
    "\"\"\"wmt18\"\"\"\n",
    "# dataset_name = 'wmt18_gpt3'\n",
    "# set_name = 'test'\n",
    "# generation_methods=['top_p_sampling']\n",
    "# models = ['gpt3']\n",
    "# metrics = ['bleu']\n",
    "\n",
    "with open(f\"./data/prepared/{dataset_name}/{set_name}/dataset.jsonl\", \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "for item in data:\n",
    "    item['candidates'] = [c for c in item['candidates'] if c['model'] in models]\n",
    "    item['candidates'] = [c for c in item['candidates'] if c['generation_method'] in generation_methods]\n",
    "    for candidate in item['candidates']:\n",
    "        candidate['scores'] = {\n",
    "            metric: candidate['scores'][metric] for metric in metrics\n",
    "        }\n",
    "backbone = \"microsoft/deberta-v3-large\"\n",
    "# backbone = \"roberta-large\"\n",
    "checkpoint_path = Path(f\"./outputs/crosscompare/{backbone}/test_{dataset_name}_PairReranker_full_comparison/\")\n",
    "predictions = torch.load(checkpoint_path / \"predictions.pt\")\n",
    "labels = torch.load(checkpoint_path / \"labels.pt\")\n",
    "\n",
    "scores = np.array([[list(c['scores'].values()) for c in item['candidates']] for item in data])\n",
    "# readjust the weight of the labels so that it's the same with scores\n",
    "for i, metric in enumerate(metrics):\n",
    "    labels[:, :, i] = labels[:, :, i] / METRIC_WEIGHTS[metric]\n",
    "if np.abs((scores - labels)).mean() < 1e-5:\n",
    "    print(\"dataset score consistent with PairReranker labels\")\n",
    "else:\n",
    "    print(np.abs((scores - labels)).mean())\n",
    "# get the ranks\n",
    "ranks = np.flip(np.argsort(scores, axis=1), axis=1)\n",
    "values = np.arange(ranks.shape[1]).reshape(1, -1, 1).repeat(ranks.shape[0], axis=0).repeat(ranks.shape[2], axis=2)\n",
    "np.put_along_axis(ranks, ranks.copy(), values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR_checkpoint_path = Path(f\"./outputs/scr/{backbone}/test_{dataset_name}_SummaReranker/\")\n",
    "SR_predictions = torch.load(SR_checkpoint_path / \"predictions.pt\")\n",
    "SR_labels = torch.load(SR_checkpoint_path / \"labels.pt\")\n",
    "# readjust the weight of the labels so that it's the same with scores\n",
    "for i, metric in enumerate(metrics):\n",
    "    SR_labels[:, :, i] = SR_labels[:, :, i] / METRIC_WEIGHTS[metric]\n",
    "if np.abs((scores - SR_labels)).mean() < 1e-5:\n",
    "    print(\"dataset score consistent with SummaReranker labels\")\n",
    "else:\n",
    "    print(np.abs((scores - SR_labels)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairreranker full comparison results\n",
      "bleu 26.05805955998502 1.7502517623363545\n",
      "cider 13.523107790338278 1.1711983887210473\n"
     ]
    }
   ],
   "source": [
    "# Pairreranker full comparison\n",
    "full_cmp_results = predictions[0]\n",
    "# full_cmp_results = torch.tanh(torch.tensor(full_cmp_results)).numpy()\n",
    "# best_idx = full_cmp_results.sum(axis=-1).argmax(-1) # pre-position comparison\n",
    "# best_idx = full_cmp_results.sum(axis=1).argmin(-1) # post-position comparison\n",
    "best_idx = (full_cmp_results.sum(axis=-1) - full_cmp_results.sum(axis=1)).argmax(-1) # pre-post-position comparison\n",
    "# best_idx = ((full_cmp_results > 0).sum(axis=-1) + (full_cmp_results < 0).sum(axis=-2)).argmax(-1) # most wins\n",
    "best_scores = scores[np.arange(len(best_idx)), best_idx]\n",
    "best_ranks = ranks[np.arange(len(best_idx)), best_idx]\n",
    "mean_scores = best_scores.mean(axis=0)\n",
    "mean_ranks = best_ranks.mean(axis=0)\n",
    "print(\"Pairreranker full comparison results\")\n",
    "for metric, mean_score, mean_rank in zip(metrics, mean_scores, mean_ranks):\n",
    "    print(metric, mean_score, mean_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SummaReranker Comparison results\n",
      "rouge1 0.4160939904698082 4.784\n",
      "rouge2 0.1810002765226232 5.13\n",
      "rougeLsum 0.378026947145419 4.841\n"
     ]
    }
   ],
   "source": [
    "# SummaReranker\n",
    "SR_best_idx = SR_predictions.argmax(-1)\n",
    "SR_best_scores = scores[np.arange(len(SR_best_idx)), SR_best_idx]\n",
    "SR_best_ranks = ranks[np.arange(len(SR_best_idx)), SR_best_idx]\n",
    "SR_mean_scores = SR_best_scores.mean(axis=0)\n",
    "SR_mean_ranks = SR_best_ranks.mean(axis=0)\n",
    "print(\"SummaReranker Comparison results\")\n",
    "for metric, mean_score, mean_rank in zip(metrics, SR_mean_scores, SR_mean_ranks):\n",
    "    print(metric, mean_score, mean_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dualfid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf261f3331a58fead54a6901059364ab3a07142c9197a9ed46d25600af580158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
