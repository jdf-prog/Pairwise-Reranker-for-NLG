# Pairwise Comparison-based Reranking for Enhancing NLG Models

## Installation
Please install the requirements using the following command:

```bash
conda create -n pcr python=3.9
conda install pytorch cudatoolkit=your_cuda_version -c pytorch
conda activate pcr
pip install -r requirements.txt
```

## Usage

### Data Preparation

#### Download the dataset
Download the dataset to the local machine using the following command. Note that the dataset will be shuffled before saved to the local machine.
```bash
cd ./src/baseline
bash download_dataset.sh
```
After that, you can get the raw dataset in `./data/raw/${dataset_name}`.

#### Candidate Generation
To generate candidates for the dataset using public checkpoint or self-trained checkpoint, please run the following command:
```bash
cd ./src/baseline
bash generate_candidates.sh
```
After generating, you can get the candidates in `./data/${dataset_name}/${split}/${decoding method}/candidates_${model_name}.pkl`.
by specifying the `dataset_name`, `split`, `decoding method` and `model_name`.

To train the half-trained model on the dataset, please run the following command:
```bash
cd ./src/baseline
bash train_baseline.sh
```
After training, you can get the half-trained checkpoint folder in `./models/${model_name}/checkpoint-${step}/`.



#### Parallel Candidate Generation

Since generating candidates takes a long time, we provide parallel generation scripts. To generate candidates using parallel scripts, please run the following command:
```bash
cd ./src/baseline
bash generate_parallel_candidates.sh
```
By changing the parameters in the script, you can generate candidates with multiple shards on multiple GPUs.
These parallel candidates will be save in multiple files `./data/${dataset_name}/${split}/${decoding method}/candidates_${model_name}.pkl.${start_idx}_${end_idx}`

To integrate the parallel candidates into a single file, please run the following command:
```bash
cd ./src/baseline
bash integrate_parallel_candidates.sh
```
This script will automatically integrate all the parallel candidate files into a single file `./data/${dataset_name}/${split}/${decoding method}/candidates_${model_name}.pkl`.
Note that the script will only integrate the parallel candidate files if all the shards are generated.
Parallel candidates files will not be deleted after integration.

In practice, we use 2 half-trained models to generate candidates on the other half of the dataset. We treat this process similar to the parallel candidate generation.
What's different the parallel candidate files are still generated by `generate_candidate.sh` instead of `generate_parallel_candidate.sh`.
By specifying the `--partition` parameter in `generate_candidate.sh`, you can get the parallel candidate files similarly, where there are only 2 shards.

#### Compute Metric Scores

After generating candidates, you can compute the metric scores using the following command:
```bash
cd ./src/baseline
bash eval_oracle.sh
```
This script will compute the metric scores for all the candidates for a specific dataset and split.
It will also output a analysis table about the oracle scores.

If you do not set `--overwrite` to be True, the script will only compute the metric scores for the candidates that have not been computed.

Besides, please set parameter `--save_prepared` to be True if you want to save it as the data for training the reranker.
The prepared data will be saved in `./data/prepared/${dataset_name}/${split}/dataset.jsonl`.
By default, the script will not save the prepared data, in order not to overwrite the existing data.

This prepared jsonline file will be the only data file used in the reranking process.

### Reranking

To train the reranker, please run the following command:
```bash
cd ./scripts
bash train_reranker.sh
```
We provide example training scripts for different datasets in our experiments.
You can also specify the parameters in the script to train the reranker on your own dataset if you have prepared the data after the steps above.

To test the reranker, please specify the parameter `--predict` to be True in the training script.
In that case, the script will automatically load the best checkpoint after finishing training and test the model on the test set.

To test the reranker with a specific checkpoint, please specify the parameter `--load_checkpoint` to be the path of the checkpoint folder.
Remember to specify `--train` to be False if you only want to test the model.


## Model architecture
![Model Architecture](./model%20arch.png)

## Our Results

## Reranker checkpoints

## Citation
